<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <!-- For viewing the page on mobile devices -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <title>
      Zachariah W Miller - Data Analysis, Physics, Software
    </title>
    <link rel="stylesheet" href="../css/about_style.css">
    <link rel="icon" type="image/png" href="../images/favicon2.png" sizes="32x32">
  </head>
  <body>
    <div id="content">
      <center>
        <div id="logo">
          <img src="../images/Logo_v2_200_by_200.png" alt="ZWM Logo">
          <div id="logoT">
            <h1>Zachariah W Miller, PhD</h1>
            <h3>Data Analysis &middot; Physics &middot; Software</h3>
            <h3>zachariah.w.miller@gmail.com</h3>
          </div>
        </div>
      </center>
      <!-- <center>
        <div id="logo">
        <img src="../images/sampleBanner.png" alt="ZWM Banner">
        </div>
        </center>-->
        <center>
          <div id="linkBar">
            <hr>
            <ul>
              <li><a href="../index.html">Home</a></li>
              <li><a href="../projects.html">Projects</a></li>
              <li><a href="../blog.html">Blog</a></li>
              <li><a href="../resume.html">Resum&egrave;</a></li>
              <li><a href="../about.html">About</a></li>
            </ul>
            <hr>
            <hr>
          </div>
        </center>
        <br>
        <main>
        <div id="article">
          <br>
          <h3>Will Joey Votto Make the Hall of Fame?</h3>
          <small>November 21, 2016 - Python and SQL</small>
          <br><br>
          Of the many things I love about baseball, perhaps the part I find most beautiful is the ability to sum the current situation of the game in one sentence. "Top of the 9th, 3 to 2 for the home team, 2 balls and no strikes, man on first." That tells you literally everything you need to know about the game (assuming you know the lingo), and that's a consequence of an extremely "state" driven game. Much like atoms, if you know the state of a few factorized portions of the game, you know the current state of the whole game. This leads to my absolute favorite part of baseball: if everything is a state, then every state can be given statistics. In practicum, that means there are statistics for everything. Want to know how many of a particular players hits are doubles? We have the stats. Want to know how often a player tries to steal a base and gets caught? We have hte stats. You get the idea.<br><br>
          At present in the MLB culture, the "stat heads" are overtaking the old-guard that believes statistics are hokum. Instead, data is revolutionizing the game as teams are using all of the data to make predictions on where a player is most likely to hit the ball or what type of pitch he's least likely to hit. Perhaps unsurprisingly, this is working. Just ask the 2016 Chicago Cubs. This battle between stats and the old-guard is perhaps most clear when people talk about the hall of fame. Stat-guys are pointing to players like my favorite player, Joey Votto of the Cincinnati Reds, and saying, "he contributes more increase in winning percentage to his team than any other player." That's a pretty dense statement with a lot of calculations hidden behind it, for example how much is getting on base worth towards winning the game? These calculations are a discussion for another time, because the point I want to make is, many of the baseball writers and announcers that are "anti-stats" are pushing back on the idea that Joey Votto belongs in the hall of fame.<br><br>
          So I thought to myself, why not see what the data says? We have this beautiful game, with tons of data recorded for every player. We have a list of who made the hall of fame. I bet I can merge these two together and predict whether Joey Votto has a shot at the hall of fame. Better yet, I can make predictions on who, among active or recently retired players, is likely to get the nod. So let's talk techniques. The first thing to do is get some data.<br><br>
          Wonderfully enough, it's not too hard to find a database. After some searching about the web, I found a database <a href="BROKEN">here</a>. Inside is the statistics for every person who's ever had an at-bat in a major league game since 1871. So let's have a look at the data:
          <img src="images/mlbData.png" class="picsWide" alt="Demo of Data Structure">
          There are a few things to unpack here. First, everyone is identified by an ID code word instead of their name. Second, we see that most of the stats are the typical stats but they're broken down by player and season, so our code must handle that. Third, we see that there are some blanks in the data, especially in the ye olde player section where the information just wasn't recorded at the time.<br><br>
          The first thing to do is plot the data and have a look for problems within the data set. In order for Python to handle this, all of the blank spots within the data have to be filled in. In this case, I filled them with -999 so that they would stick out on the plots. Shown below, I've plotted the 2D correlations between the different types of data. This serves two purposes: 1) outliers will show up along each axis and 2) it allows me to identify any information that is one-to-one correlated (redundant information) such that in the future I'm not double counting a type of statistic. The code I wrote actually produces almost 300 plots showing the correlations between each statistics with each other statistic, a few interesting examples are shown below (with the -999 outliers removed for viewing purposes).
          <img src="images/mlbDataCorrelations.png" class="picsWide" alt="Demo of Data Correlations">
          On the top left we see data that shows almost no correlations. Someone with 100 AB in a season is just as likely to have 5 IBB (intentional walks) as someone with 400 AB in a season. We also see that there  On the top right we see data that is exteremely correlated, that's because these are two numbers that measure how many times a player comes to the plate to face a pitcher, the only difference is whether to include certain types of events in the counting. However, these events are fairly rare. Thus, we don't really need to include both of these features in our analysis (at least not necessarily, we can try with and without and if we find the data needs to be simplified, this is a place to cut a feature). The bottom left is perhaps the most interesting plot to consider. We see that there is a very loose correlations between at-bats and average (with average being one of the most commonly used metrics for discussing a baseball players greatness). However, that correlation doesn't really start until over 100 ABs. That's because baseball is a game of failure. The greatest hitters of all time still fail 70% of the time. That means a large sample is necessary to truly find out how good a player is. With only 100 ABs, we still can't tell much about a hitter. Thus, for this analysis, we are going to place a requirement that more than 100 ABs occur in a season for the season to be considered. Finally, on the bottom right I've shown a plot that shows a double band structure. This is a unique looking plot that shows us there isn't much of a correlations between OBP (on-base percentage) and SO (strike outs), but there does seem clearly be two types of players, those with low OBP and relatively low strike outs, and "the rest." After checking into it, the lower band tends to be players with only a few ABs.<br><br>
          For the sake of brevity, let's jump to the interesting part - machine learning. For each player I created a Python dictionary that holds all of his stats, separated by seasons, and whether or not he has made the hall of fame. I removed pitchers from the data, because pitchers make it to the hall of fame for their arm and not their bats, so they would skew the results. I then separated the players into three groups: "Eligible and retired long enough to have been considered for the hall of fame", "Not yet retired or not retired long enough to be eligible", and "ineligible" based on the Hall of Fame requirements for entry. From this, I took the first group and split it further, placing 70% into a "training" data set for my machine learning algorithms and 30% into a "testing" group so I can see how well my algorithm can do at determining if a player actually makes the hall of fame. Then, after testing the algorithm I can apply it to the group of "modern players" that we want to make predictions about.<br><br>
          This is simple enough in theory, however there are some subtleties I'd like to discuss. First, only ~2% of players that are eligible make the hall of fame, so this data is dominated by "no" answers to the question "did he make the hall of fame?" That means the algoritmh would be ~98% accurate if it just always said no. It also means we need to be careful about our training data. The predictions are going to vary greatly if the only person in the training data that makes the hall of fame is Babe Ruth vs Barry Larkin. To overcome this, I've used a bootstrap method combined with a method of forcing 70% of "yes's" and 70% of "no's" to go into the training data individually instead of just 70% of all players. This means my training data will always have yes's and no's to train on. For the bootstrap method, I've done a method of sampling the data to build the training set many 1000s of times and then constructing a probability of making the hall of fame based on all of these samples. This removes bias based on what players the data is trained with.<br><br>
          With just these assumptions, we can apply various machine learning algorithms and look at the probability of each player making the hall of fame. For each iteration of the bootstrap, we can plot the probability that a player makes the hall of fame. Organized from low-to-high probability, that gives a plot like this.
          <div style="width:70%; margin:0 auto"><center><img src="images/ProbabilityvsNumber.png" class="picsWide" alt="Hall of Fame Probability"></center></div>
          This plot is excellent for many reasons. First, it shows that our method does have discriminatory power. The vast majority of players have a less than 20% chance, and the group of players that do make it are fairly concentrated in the one peak. It also shows us that using the default &gt;50% = HOF is not a the best method, since the data is tightly grouped. Instead, we can use an optmization method with our "test" data that we separated out from the training data. If we apply a shifting probability threshold to make the decision of yes/no hall of fame, we can slide that threshold around on each iteration of the data to maximize the ability to the correctly identify hall of famers. This method is implement, optimizing the F1 score (a measure of machine learning model accuracy). This is done on a "per training data" level, in order to maximize the accuracy of each iteration.<br><br>
          Another thing to consider in the model is whether we want to just look at accrued stats or rate of accrual. In my first model, I naively just summed up each players stats and tried to make a prediction. However, that heavily biases the result against players that have only been in the league a few years. It's simply not feasible that a player with only 5 years in the league will have the accrued stats of a hall of famer with 12 years in the league. Thus, I switched to a model that accounts for the rate of accrual for a player, calculating each players "hits/season" or "walks/season." Then I re-trained the model.<br><br>
          Finally, we need to consider which model to use. For all models, it's best to regularize the data such that the mean value for each feature is 0 and the standard deviation is 1. This regularization is applied to the data for all models, in order to obey the "rules" of the models. Then I tried many different models: <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a>,<a href="https://en.wikipedia.org/wiki/Decision_tree"> decisions trees</a>, <a href="https://en.wikipedia.org/wiki/Gradient_boosting">boosted decision trees</a>, and <a href="https://en.wikipedia.org/wiki/Random_forest">random forest</a>. I found the random forest to give the most accurate results in the testing data out of these models, so the results shown here are from that model.<br><br>
          <table>
            <tr><th>First Name</th><th>Last Name</th><th>HOF Probability</th></tr>
            <tr><td>Scott</td><td>Rolen</td><td>0.004</td></tr>
            <tr><td>Paul</td><td>Konerko</td><td>0.104</td></tr>
            <tr><td>A.J.</td><td>Pollock</td><td>0.001</td></tr>
            <tr><td>Aramis</td><td>Ramirez</td><td>0.006</td></tr>
            <tr><td>Lance</td><td>Berkman</td><td>0.768</td></tr>
            <tr><td>Justin</td><td>Upton</td><td>0.004</td></tr>
            <tr><td>Ichiro</td><td>Suzuki</td><td>1.0</td></tr>
            <tr><td>Ryan</td><td>Braun</td><td>0.934</td></tr>
            <tr><td>Jose</td><td>Bautista</td><td>0.174</td></tr>
            <tr><td>Vladimir</td><td>Guerrero</td><td>0.998</td></tr>
            <tr><td>Johnny</td><td>Damon</td><td>0.611</td></tr>
            <tr><td>Mark</td><td>Teixeira</td><td>0.308</td></tr>
            <tr><td>Andre</td><td>Ethier</td><td>0.002</td></tr>
            <tr><td>Derek</td><td>Jeter</td><td>1.0</td></tr>
            <tr><td>Prince</td><td>Fielder</td><td>0.638</td></tr>
            <tr><td>Adrian</td><td>Gonzalez</td><td>0.812</td></tr>
            <tr><td>Dustin</td><td>Pedroia</td><td>0.882</td></tr>
            <tr><td>Justin</td><td>Morneau</td><td>0.005</td></tr>
            <tr><td>Carlos</td><td>Santana</td><td>0.002</td></tr>
            <tr><td>Michael</td><td>Young</td><td>0.995</td></tr>
            <tr><td>Ian</td><td>Kinsler</td><td>0.851</td></tr>
            <tr><td>Dan</td><td>Uggla</td><td>0.033</td></tr>
            <tr><td>Chris</td><td>Davis</td><td>0.007</td></tr>
            <tr><td>Brian</td><td>McCann</td><td>0.002</td></tr>
            <tr><td>Adam</td><td>Jones</td><td>0.885</td></tr>
            <tr><td>Adam</td><td>Dunn</td><td>0.529</td></tr>
            <tr><td>Nick</td><td>Swisher</td><td>0.001</td></tr>
            <tr><td>Jose</td><td>Altuve</td><td>0.168</td></tr>
            <tr><td>Robinson</td><td>Cano</td><td>0.997</td></tr>
            <tr><td>Dexter</td><td>Fowler</td><td>0.003</td></tr>
            <tr><td>Albert</td><td>Pujols</td><td>0.965</td></tr>
            <tr><td>Yunel</td><td>Escobar</td><td>0.001</td></tr>
            <tr><td>Victor</td><td>Martinez</td><td>0.055</td></tr>
            <tr><td>Freddie</td><td>Freeman</td><td>0.61</td></tr>
            <tr><td>Jimmy</td><td>Rollins</td><td>0.95</td></tr>
            <tr><td>Billy</td><td>Butler</td><td>0.599</td></tr>
            <tr><td>Hanley</td><td>Ramirez</td><td>0.054</td></tr>
            <tr><td>Bobby</td><td>Abreu</td><td>0.239</td></tr>
            <tr><td>Mark</td><td>Reynolds</td><td>0.002</td></tr>
            <tr><td>Chris</td><td>Carter</td><td>0.009</td></tr>
            <tr><td>Mark</td><td>Trumbo</td><td>0.005</td></tr>
            <tr><td>Edgar</td><td>Renteria</td><td>0.014</td></tr>
            <tr><td>Elvis</td><td>Andrus</td><td>0.955</td></tr>
            <tr><td>Matt</td><td>Holliday</td><td>0.93</td></tr>
            <tr><td>Chipper</td><td>Jones</td><td>0.975</td></tr>
            <tr><td>Jason</td><td>Giambi</td><td>0.122</td></tr>
            <tr><td>Jason</td><td>Heyward</td><td>0.001</td></tr>
            <tr><td>Josh</td><td>Hamilton</td><td>0.002</td></tr>
            <tr><td>Jose</td><td>Reyes</td><td>0.015</td></tr>
            <tr><td>Joey</td><td>Votto</td><td>0.93</td></tr>
            <tr><td>Travis</td><td>Hafner</td><td>0.002</td></tr>
            <tr><td>Evan</td><td>Longoria</td><td>0.464</td></tr>
            <tr><td>Andrew</td><td>McCutchen</td><td>0.939</td></tr>
            <tr><td>Eric</td><td>Hosmer</td><td>0.669</td></tr>
            <tr><td>Curtis</td><td>Granderson</td><td>0.266</td></tr>
            <tr><td>David</td><td>Wright</td><td>0.139</td></tr>
            <tr><td>Alex</td><td>Rodriguez</td><td>0.727</td></tr>
            <tr><td>Nelson</td><td>Cruz</td><td>0.001</td></tr>
            <tr><td>Torii</td><td>Hunter</td><td>0.097</td></tr>
            <tr><td>Adrian</td><td>Beltre</td><td>0.463</td></tr>
            <tr><td>Mike</td><td>Trout</td><td>0.809</td></tr>
            <tr><td>Bryce</td><td>Harper</td><td>0.045</td></tr>
            <tr><td>Carlos</td><td>Beltran</td><td>0.008</td></tr>
            <tr><td>Jim</td><td>Thome</td><td>0.34</td></tr>
            <tr><td>Pat</td><td>Burrell</td><td>0.017</td></tr>
            <tr><td>Nick</td><td>Markakis</td><td>0.961</td></tr>
            <tr><td>Adam</td><td>LaRoche</td><td>0.001</td></tr>
            <tr><td>Magglio</td><td>Ordonez</td><td>0.845</td></tr>
            <tr><td>Josh</td><td>Donaldson</td><td>0.479</td></tr>
            <tr><td>Brandon</td><td>Phillips</td><td>0.628</td></tr>
            <tr><td>Giancarlo</td><td>Stanton</td><td>0.181</td></tr>
            <tr><td>Kevin</td><td>Kouzmanoff</td><td>0.002</td></tr>
            <tr><td>Jay</td><td>Bruce</td><td>0.152</td></tr>
            <tr><td>Alexei</td><td>Ramirez</td><td>0.432</td></tr>
            <tr><td>Carlos</td><td>Pena</td><td>0.02</td></tr>
            <tr><td>Carlos</td><td>Lee</td><td>0.607</td></tr>
            <tr><td>Matt</td><td>Kemp</td><td>0.002</td></tr>
            <tr><td>Austin</td><td>Jackson</td><td>0.001</td></tr>
            <tr><td>Anthony</td><td>Rizzo</td><td>0.006</td></tr>
            <tr><td>Paul</td><td>Goldschmidt</td><td>0.334</td></tr>
            <tr><td>Alcides</td><td>Escobar</td><td>0.001</td></tr>
            <tr><td>Miguel</td><td>Cabrera</td><td>0.973</td></tr>
            <tr><td>Todd</td><td>Helton</td><td>0.983</td></tr>
            <tr><td>Nolan</td><td>Arenado</td><td>0.098</td></tr>
            <tr><td>Juan</td><td>Pierre</td><td>0.33</td></tr>
            <tr><td>Buster</td><td>Posey</td><td>0.301</td></tr>
            <tr><td>David</td><td>Ortiz</td><td>0.717</td></tr>
            <tr><td>Vernon</td><td>Wells</td><td>0.063</td></tr>
            <tr><td>Starlin</td><td>Castro</td><td>0.536</td></tr>
            <tr><td>Ryan</td><td>Howard</td><td>0.697</td></tr>
            <tr><td>Edwin</td><td>Encarnacion</td><td>0.01</td></tr>
            <tr><td>Brian</td><td>Dozier</td><td>0.027</td></tr>
            <tr><td>Hideki</td><td>Matsui</td><td>0.006</td></tr>
            <tr><td>Chase</td><td>Utley</td><td>0.004</td></tr>
            <tr><td>Matt</td><td>Carpenter</td><td>0.824</td></tr>
            <tr><td>Ian</td><td>Desmond</td><td>0.487</td></tr>
            <tr><td>Miguel</td><td>Tejada</td><td>0.333</td></tr>
            <tr><td>Alfonso</td><td>Soriano</td><td>0.074</td></tr>
            <tr><td>Jacoby</td><td>Ellsbury</td><td>0.006</td></tr>
            <tr><td>Joe</td><td>Mauer</td><td>0.664</td></tr>
          </table>
          <br>
          The first, and most positive, note is that most of these players are well-known to the casual baseball fan. The algorithm is making predictions that agree with most baseball experts. For instance, Chipper Jones is widely believed to be a shoo-in for the hall of fame, and our model gives him a 98% chance of making it. The same for Derek Jeter. It also says that most of the current excellent players who are on pace to be shoo-ins, like Miguel Cabrera, Albert Pujols, have high probabilities. Perhaps even more encouraging, is that I don't see many names on here that I don't recognize (me being slightly more baseball-rabid than the normal fan). So the model is also accurately removing players that have no name recognition for being great. "Oh hey, I recognize that guy" isn't a metric I'd put on my resume, but it's definitely reassuring that the model is being successful.<br><br>
          I'd also like to spend a few sentence on the limitations of this model. There are many things this model doesn't account for, but the two biggest ones I think are playing position, defense (as a whole), and home-field advantage. The position a player is known for makes a big difference in the hall of fame. Someone who is an excellent shortstop will likely have a lower barrier for entrance in terms of hitting statistics than a first baseman (a position associated with power hitters). For instance Buster Posey is one of the most heralded catchers of the past generation, yet only has a 30% chance according to this model. That's because this model doesn't know he's a catcher (and one of the best defensive catchers of all time. Todd Helton is given a high probability of making the hall of fame, but the model doesn't know he played the majority of his games in Denver, where the stadium is well-known for inflating offensive numbers because the atmosphere is much thinner at 5280 ft above sea-level. These are all secondary effects that need more study, but I think they are small effects and the results of this analysis can still be quite reliable and useful.<br><br>
          So what does the model have to say about my man Joey Votto? It gives him a 93% chance, which puts him on par with the Albert Pujolses of the world. Frankly, that shouldn't be surprising given the excellent career that Joey Votto has had so far, but it's nice to dig in to the data and see that it's not just watching him that says he's great; he actually is on pace to sit among the legends of the game. It's also really fun and interesting to look through some of the other names that show up. Todd Helton is a great example of a forgotten great hitter; and someone that I didn't expect to find on this list, but after doing this analysis and looking into his stats, he does belong here. Ryan Howard is someone I had considered before doing this, but expected him not to make the list because he's generally seen as a massive failure. However, he's actually had an excellent career, but the tale in the media about him is that he never lived up to the huge contract he was given. There are tales like this throughout the list, and this study has shown that the media can often miss the great careers when they focus so much on building stories around players.<br><br>
          For future development, it would be great to add in a defensive position feature and to re-try this using a deep learning algorithm that might be able to pick out stronger correlations between multiple variables and the Hall of Fame chance. Perhaps there's a strong link between playing lots of games AND having a high OBP that is a better predictor of HOF chances. At present, this model may not make that connection reliably and a neural network might. <br><br>
          <center><h4>Code available <a href="https://github.com/zwmiller/LearningPython/MLBAnalysis">here.</a></h4></center>
        </div>
        <span class="clear"></span>
        </main>
        <footer>
          <center>
            <div id="footer">
              <hr>
              <div id="mediaLinks">
                <ul>
                  <li><a href="https://www.linkedin.com/in/zach-miller-aab09594"><img src="../images/linkedinLogo.png"></a></li>
                  <li><a href="https://github.com/zwmiller"><img src="../images/githubLogo2.png"></a></li>
                  <li><a href="https://twitter.com/zaglamir"><img src="../images/twitterLogo.png"></a></li>
                  <li><a href="https://www.facebook.com/zachmilller"><img src="../images/facebookLogo.png"></a></li>
                  <li><a href="https://www.youtube.com/zaglamir"><img src="../images/youtubeLogo.png"></a></li>
                </ul>
                <br>
                <small>Copyright &copy; 2016 Zachariah Miller</small>
                <br>
                <br>
              </div>
          </center>
        </footer>
        </content>
  </body>
</html>

